input {
	beats {
		port => xx
		host => ["xx"]
	}
}

filter {
	# parse for md
	if [mode] == "out" {
		grok {
			patterns_dir   => "C:\Logstash\config\patterns.d"
			match          => [ "message", "%{MD_DATE:Date}.*? .%{MD_ID:ID}. %{MD_FLAGS:Flags} %{GREEDYDATA:Extra}|%{MD_DATE:Date}.*? .%{MD_ID:ID}. %{GREEDYDATA:Extra}" ]
			tag_on_failure => [ "_grok_md_parse_nomatch" ]
			add_tag        => [ "_grok_out_success" ]
			add_field      => [ "direction", "OUT" ]
		}
	} else if [mode] == "in" {
		grok {
			patterns_dir   => "C:\Logstash\config\patterns.d"
			match          => [ "message", "%{MD_DATE:Date}.*? .%{MD_ID:ID}. %{MD_FLAGS:Flags} %{GREEDYDATA:Extra}|%{MD_DATE:Date}.*? .%{MD_ID:ID}. %{GREEDYDATA:Extra}" ]
			tag_on_failure => [ "_grok_md_parse_nomatch_in" ]
			add_tag        => [ "_grok_in_success" ]
			add_field      => [ "direction", "IN" ]
		}
	}

		# preformat log received from filebeat input
		if [direction] == "OUT" {
			grok {
				patterns_dir   => "C:\Logstash\config\patterns.d"
				match          => [ "message", "%{MD_OUT_PARSE}" ]
				tag_on_failure => [ "_grok_md_parse_nomatch_out" ]
			}
		} else if [direction] == "IN" {
			grok {
				patterns_dir   => "C:\Logstash\config\patterns.d"
				match          => [ "message", "%{MD_IN_PARSE}" ]
				tag_on_failure => [ "_grok_md_parse_nomatch_out" ]
			}
		}

	# Do some data type conversions
	mutate {
		rename => [
			"[agent][hostname]", "Host",
			"domain_from", "Domain From",
			"domain_to", "Domain To",
			"md_size", "Size",
			"md_remote_ip", "Remote IP",
            "md_remote_port", "Remote Port",
            "md_client_ip", "Client IP",
            "md_client_port", "Client Port",
			"md_client_port", "Client Port",
			"direction", "Direction"
		]
		convert => [
            "Remote Port", "integer",
            "Client Port", "integer",
			"Size", "integer"
        ]
		remove_field => [
            "[agent][ephemeral_id]", "[agent][name]", "[agent][id]", "[agent][type]", "[agent][version]", "domain", "dsn", "ecs", "mode", "host", "message", "input", "log", "offset"
        ]
	}

	aggregate {
	task_id => "%{ID}"
	code => "
		map['Date'] ||= event.get('Date')
		map['Size'] ||= event.get('Size')
		map['ID'] ||= event.get('ID')
		map['From'] ||= event.get('From')
		map['Domain From'] ||= event.get('Domain From')
		map['To'] ||= []
		if  event.get('To')
			map['To'] << event.get('To')
		end
		map['Subject'] ||= event.get('Subject')
		map['Message ID'] ||= event.get('Message ID')
		map['Remote IP'] ||= event.get('Remote IP')
		map['Remote Port'] ||= event.get('Remote Port')
		map['Client IP'] ||= event.get('Client IP')
		map['Client Port'] ||= event.get('Client Port')
		map['Action'] ||= event.get('Action')
		map['Direction'] ||= event.get('Direction')
		"
		map_action => "create_or_update"
		push_map_as_event_on_timeout => true
		timeout => 60
		timeout_code => "
			event.set('To', event.get('To'))
		"
		timeout_tags => [ "MD" ]
	}

	mutate {
		split => { "To" => "," }
	}
}

output {
	if "MD" in [tags] {
		elasticsearch {
			hosts => [
      "https://xx1:9200",
      "https://xx2:9200"
			]
			user => "xx"
			password => "xx"
			cacert => 'C:\Logstash\ca\ca.crt'
			ssl => 'true'
			ssl_certificate_verification => 'false'
			manage_template => false
			index => "xx-xx-delivery-xx-%{+MM.YYYY}"
		}
			stdout { codec => rubydebug }
	}
}
